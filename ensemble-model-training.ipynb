{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\Development\\kdg\\Data\\5\\ensembles-model-training\\.venv\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Load MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializes and trains three different classifiers (Random Forest, MLP, and SVM) using the training set (`X_train` and `y_train`) and evaluates their performance on the validation set (`X_val`). The accuracy scores are then printed for each classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9648571428571429\n",
      "MLP Accuracy: 0.9595714285714285\n",
      "SVM Accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=100, random_state=42)\n",
    "svm_classifier = SVC(probability=True, random_state=42)\n",
    "\n",
    "# Train classifiers\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "rf_val_preds = rf_classifier.predict(X_val)\n",
    "mlp_val_preds = mlp_classifier.predict(X_val)\n",
    "svm_val_preds = svm_classifier.predict(X_val)\n",
    "\n",
    "# Print accuracy for each classifier\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_val, rf_val_preds))\n",
    "print(\"MLP Accuracy:\", accuracy_score(y_val, mlp_val_preds))\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_val, svm_val_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest (RF): Achieved an accuracy of 96.49%. Random Forest is an ensemble learning method that builds multiple decision trees and merges their predictions. It is known for handling complex relationships in data and providing robust performance.\n",
    "\n",
    "MLP (Multi-Layer Perceptron): Achieved an accuracy of 95.96%. MLP is a type of neural network with one hidden layer of 100 neurons. Although neural networks can capture intricate patterns, this specific configuration might need further tuning for better performance.\n",
    "\n",
    "SVM (Support Vector Machine): Achieved the highest accuracy of 97.50%. SVM is a powerful classification algorithm that finds the optimal hyperplane to separate different classes. The high accuracy suggests that the SVM was effective in discriminating between the classes in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Ensemble Accuracy: 0.977\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Create a voting ensemble (soft or hard voting)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('rf', rf_classifier),\n",
    "    ('mlp', mlp_classifier),\n",
    "    ('svm', svm_classifier)],\n",
    "    voting='soft')  # Use 'hard' for hard voting\n",
    "\n",
    "# Train the voting ensemble\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "voting_test_preds = voting_classifier.predict(X_test)\n",
    "\n",
    "# Print accuracy for the ensemble\n",
    "print(\"Voting Ensemble Accuracy:\", accuracy_score(y_test, voting_test_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creates a voting ensemble using scikit-learn's `VotingClassifier` class. It combines predictions from three previously trained classifiers (Random Forest, MLP, and SVM) and evaluates the ensemble's performance on the test set.\n",
    "\n",
    "The voting ensemble achieved an accuracy of 97.70% on the test set. This accuracy is higher than the individual classifiers' accuracies on the validation set, indicating that combining the predictions from multiple classifiers improved overall performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Get predictions on validation set\n",
    "rf_val_preds = rf_classifier.predict(X_val)\n",
    "mlp_val_preds = mlp_classifier.predict(X_val)\n",
    "svm_val_preds = svm_classifier.predict(X_val)\n",
    "\n",
    "# Create a new training set for the blender\n",
    "blender_X_train = np.column_stack((rf_val_preds, mlp_val_preds, svm_val_preds))\n",
    "blender_y_train = y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Ensemble Accuracy: 0.9627142857142857\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a blender classifier (you can use any classifier, e.g., Logistic Regression)\n",
    "blender_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the blender\n",
    "blender_classifier.fit(blender_X_train, blender_y_train)\n",
    "\n",
    "# Get predictions from individual classifiers on the test set\n",
    "rf_test_preds = rf_classifier.predict(X_test)\n",
    "mlp_test_preds = mlp_classifier.predict(X_test)\n",
    "svm_test_preds = svm_classifier.predict(X_test)\n",
    "\n",
    "# Create input for blender from test set predictions\n",
    "blender_X_test = np.column_stack((rf_test_preds, mlp_test_preds, svm_test_preds))\n",
    "\n",
    "# Get predictions from the blender on the test set\n",
    "blender_test_preds = blender_classifier.predict(blender_X_test)\n",
    "\n",
    "# Print accuracy for the stacking ensemble\n",
    "print(\"Stacking Ensemble Accuracy:\", accuracy_score(y_test, blender_test_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stacking ensemble achieved an accuracy of 96.27% on the test set. This accuracy is comparable to or slightly below the accuracy achieved by the individual classifiers and the voting ensemble.\n",
    "\n",
    "Stacking aims to leverage the strengths of diverse models, and its success can depend on factors such as the choice of base classifiers, the complexity of the problem, and the quality of predictions from individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "## Further Analysis and Considerations:\n",
    "Stacking can be sensitive to the choice of base models and the diversity of their predictions. Experimenting with different base models or tweaking hyper-parameters might lead to improvements.\n",
    "\n",
    "The performance of the stacking ensemble might benefit from additional fine-tuning, such as adjusting hyperparameters of the blender model or exploring different types of blenders.\n",
    "\n",
    "It's important to note that the performance of stacking may vary across datasets. The dataset characteristics, the number of instances, and the inherent complexity of the problem can all impact the effectiveness of ensemble methods.\n",
    "\n",
    "While stacking didn't significantly outperform the other ensemble methods in this specific case, it remains a powerful technique that can yield improvements in various scenarios, particularly when dealing with diverse base models.\n",
    "\n",
    "## Final Thoughts:\n",
    "The choice between different ensemble methods (voting, stacking) and individual classifiers depends on the specific characteristics of the dataset and the nature of the problem being addressed.\n",
    "\n",
    "Experimentation, hyperparameter tuning, and understanding the strengths and weaknesses of each model are essential steps in building effective ensembles.\n",
    "\n",
    "The reported accuracy scores provide valuable insights into the relative performance of different models and ensembles, helping guide further refinement and exploration in the machine learning workflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
